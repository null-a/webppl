// LDA
//
// Run this with:
// LOG_LEVEL=1 ./webppl examples/ldaVariational-ndg.wppl

// Known issues:

// It's not uncommon to see the following error:
// "AssertionError: Expected marginal to be normalized."

// I'm not sure what causes this, but one guess is that it's possible
// to sample values on the boundary of the simplex from the logistic
// normal. Such values are outside of the support of the Dirichlet, so
// scoring them may lead to the gradient blowing up.



/*
comments on variational from my explorations (these belong somewhere else):

-Q: if we use LR with differentiable sampler, do we get reparam? A: no.
-need to understand what trick yields LR from non-differentiable sampler.

-can we relax or enumerate all discrete vars, and then add discrete randomization only at conditionals? i.e. we should only have to incur the variance hit for structure change.
 -is the current method actually correct when there’s structure change?

-how can / should we use the VI guide for SMC? get guide params, use in withImportanceDist?
-maybe the core variational routine should just return a parameters / sampler object?
 -arguably variational is not a standalone inference method, but only finds guide params for use in importance sampling, SMC, etc.
-we should probably unify proposers (MH), importance distributions (SMC), and guides (Variational).

-should make better library for mini-batches / updates to parallel sub-models.
  -should allow random mini-batch as well as cycling.
  -the minibatch forEach should take batch size (rather than being global) to facilitate nested batching.

-what does .bind(this) do? it’s all over the code! there must be a clearer way to write these inference loops... macros to the rescue?

-for LDA:
 -need to fix the simplex-edge bug.
 -does the nerual guide work?
 -build the cocolab abstracts corpus!
   -`$(“.abstract”).text().split(“Abstract:”)` in the pubs page gets raw array of abstracts.
   -then need to tokenize and clean up…
   -throw out stop words and those with small count across corpus?

*/

// This seem to happen more often with larger step sizes.

var ones = function(dims) { zeros(dims).add(1); };

// TODO: Add these helpers to header.wppl if useful.
var forEach2 = function(xs, f) {
  map(function(i) {
    return f(xs[i], i);
  }, _.range(xs.length));
  return;
};

var times = function(n, f) {
  forEach2(_.range(n), f);
};

var paramInit = function(val, options) {
  return paramChoice(deltaERP, [val], options);
};

// Hack to call erp logistic without member expression. Useful for
// approximating dirichlet with delta...
var erp = { logistic: logistic };

var initSigma = 0.05;
var hDim = 10;


var lda = function(corpus, vocabSize, numTopics, alpha, eta) {

  var topics = repeat(numTopics, function() {
    var q = sampleGuide(logisticNormalERP, [
      paramInit(zeros([vocabSize - 1, 1]), { prefix: 'topic_mu' }),
      ad.tensor.exp(paramInit(zeros([vocabSize - 1, 1]), { prefix: 'topic_log_sigma' }))
    ]);

    return sample(dirichletERP, [eta], { guideVal: q }); // beta_k
  });

  var W0 = paramChoice(matrixGaussianERP, [0, initSigma, [hDim, vocabSize]], { name: 'W0' });
  var W1 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, hDim]], { name: 'W1' });
  var W2 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, hDim]], { name: 'W2' });
  var b0 = paramChoice(matrixGaussianERP, [0, initSigma, [hDim, 1]], { name: 'b0' });
  var b1 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, 1]], { name: 'b1' });
  var b2 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, 1]], { name: 'b2' });

  forEach2(corpus, function(doc) {

    //neural guide: use a NN with the word-count vector for the document as input that spits out the props_mu and props_log_sigma for the logisticNormal guide.
    //a fancy modern way to do this woulld be to convert words to vectors via a (learned) embedding and then use an RNN to build the doc vector from which we get mu and sigma.... not sure this would actually do better than the count-vector method.
    var docTensor = ad.tensor.reshape({data:doc, length:doc.length},[vocabSize,1])
    var h = ad.tensor.tanh(ad.tensor.add(ad.tensor.dot(W0, docTensor), b0));
    var mu = ad.tensor.add(ad.tensor.dot(W1, h), b1);
    var log_sigma = ad.tensor.exp(ad.tensor.add(ad.tensor.dot(W2, h), b2));
    var q = sampleGuide(logisticNormalERP, [mu, ad.tensor.exp(log_sigma)]);

    // //version without the NN guide:
    // var q = sampleGuide(logisticNormalERP, [
    //   paramInit(zeros([numTopics - 1, 1]), { prefix: 'props_mu' }),
    //   ad.tensor.exp(paramInit(zeros([numTopics - 1, 1]), { prefix: 'props_log_sigma' }))
    // ]);

    var topicDist = sample(dirichletERP, [alpha], { guideVal: q }); // theta_d


    // Note, that this forEach should not use mini-batches.
    // Well.. it's not crazy to do so, especially if we have large documents and
    // don't reduce to counts. (working over raw words instead of counts will be necessary if
    // we want to explore topic/sequence models.)
    forEach2(doc,
      function(count, word) {

      //note: when in the situation where most docs don't contain most words,
      //it'll be faster to compute here the marginal score for each observed word.
      var marginal = Enumerate(function() {
        var z = sample(discreteERP, [topicDist]);
        var topic = topics[z];
        return sample(discreteERP, [topic]);
      });

      //this version is much faster (and prettier):
      factor(count*marginal.score([], word));

    });

  });

  return topics;

};


// Each document is represented by an array of word counts. Therefore
// doc.length == vocabSize, and sum(doc) = no. of words in doc.

var bars = readJSON('bars2.json');

var vocabSize = 4; // V
var numTopics = 4; // K

// Parameter for prior on topic proportions.
var alpha = Vector(repeat(numTopics, constF(0.1)));
// Parameter for prior on topics.
var eta = Vector(repeat(vocabSize, constF(0.1)));



var marginal = Variational(function() {
  return lda(bars, vocabSize, numTopics, alpha, eta);
}, {
  optimizer: { adagrad: { stepSize: 0.1 } },
  //optimizer: { adam: { stepSize: 0.01 } },
  steps: 100,
  samplesPerStep: 1,
  returnSamples: 1,
  callback: function(i, parameters) {
    var mus = [parameters.topic_mu0,
               parameters.topic_mu1,
               parameters.topic_mu2,
               parameters.topic_mu3];

    // var sigmas = [parameters.topic_log_sigma0,
    //               parameters.topic_log_sigma1,
    //               parameters.topic_log_sigma2,
    //               parameters.topic_log_sigma3];

    // The topics used to generate the bars2 data are 2x2 horizontal
    // and vertical stripes.

    // [.5, .5, .0, .0]
    // [.0, .0, .5, .5]
    // [.5, .0, .5, .0]
    // [.0, .5, .0, .5]

    display(_.map(mus, logistic));
    //display(_.map(sigmas, ad.tensor.exp));
  }
});

//map(function(t) { return ad.value(t).toFlatArray(); }, marginal.support()[0]);
//writeJSON('topics.json', map(function(topic) { topic.toFlatArray(); }, marginal.support()[0]));

'done';
