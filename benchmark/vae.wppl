// run with:
//   webppl benchmark/vae.wppl --require fs -- --mnist-path path/to/mnist.json

// code to generate mnist can be found here:
// https://github.com/null-a/webppl-nn/tree/master/examples/data

var load_mnist = function(path) {
  display('loading mnist...');
  var images = JSON.parse(fs.readFileSync(path));
  assert.ok(images.length === 60000);
  // Return an array of tensors because rather than a single tensor as
  // this is what `mapData` currently expects.
  var arr = map(function(img) {
    return tf.tensor(img, [784, 1], 'bool');
  }, images.slice(0, 50000)); // typical training set
  assert.ok(arr.length === 50000);
  display('done');
  return {images: arr, N: arr.length, xDim: arr[0].shape[0]};
};

var dummy_data = function(N, xDim) {
  display('generating dummy data...');
  var ps = T.mul(ones([xDim, 1]), 0.1);
  var images = repeat(N, function() {
    // I'm continuing to use WebPPL's convention of representing
    // vectors as n-by-1 tensors.
    return multivariateBernoulli(ps);
  });
  display('done');
  return {images, N, xDim};
};

var dataObj = argv['mnist-path'] ?
    load_mnist(argv['mnist-path']) :
    dummy_data(argv['N'] || 10, argv['xDim'] || 10);

var images = dataObj.images;
var N = dataObj.N;
var xDim = dataObj.xDim;
var zDim = argv['zDim'] || 10;
var hDim = argv['hDim'] || 10;
var batchSize = argv['batchSize'] || 10;
var numSteps = argv['numSteps'] || 10;
var stepSize = argv['stepSize'] || 0.001;
var adBackend = _.has(global, 'tf') ? 'tfjs-' + tf.ENV.backendName : 'adnn';
var condition = {N, xDim, hDim, zDim, batchSize, numSteps, stepSize, adBackend};
display(condition);

var decode = function(z) {
  var W0 = param({name: 'W0d', dims: [hDim, zDim]});
  var b0 = param({name: 'b0d', dims: [hDim, 1]});
  var W1 = param({name: 'W1d', dims: [xDim, hDim]});
  var b1 = param({name: 'b1d', dims: [xDim, 1]});
  var hid = T.tanh(T.add(T.dot(W0, z), b0));
  var ps = T.sigmoid((T.add(T.dot(W1, hid), b1)));
  return ps;
};

var zPrior = TensorGaussian({mu: 0, sigma: 1, dims: [zDim, 1]});

var zGuideParams = function(image) {
  var W0 = param({name: 'W0e', dims: [hDim, xDim]});
  var b0 = param({name: 'b0e', dims: [hDim, 1]});
  var Wm = param({name: 'W1e', dims: [zDim, hDim]});
  var bm = param({name: 'b1e', dims: [zDim, 1]});
  var Ws = param({name: 'W2e', dims: [zDim, hDim]});
  var bs = param({name: 'b2e', dims: [zDim, 1]});
  var hid = T.tanh(T.add(T.dot(W0, image), b0));
  var mu = T.add(T.dot(Wm, hid), bm);
  // Use `exp` for portability.
  var sigma = T.exp(T.add(T.dot(Ws, hid), bs));
  return {mu, sigma};
};

var vae = function(obsImg) {
  var z = sample(zPrior, {guide() {
    return DiagCovGaussian(zGuideParams(obsImg));
  }});
  var img = observe(MultivariateBernoulli({ps: decode(z)}), obsImg);
  return {z, img};
};

var model = function(data, batchSize) {
  return mapData({data, batchSize}, vae);
};

// sample from the prior:
// var sample = vae();
// sample.z.print();
// sample.img.print();

// run the guide on the first input:
// var params = zGuideParams(images[0]);
// params.mu.print();
// params.sigma.print();

//var samples = forward(function() { return model(images.slice(0, 2), 2); });
// var samples = forwardGuide(function() { return model(images.slice(0, 2), 2); });
// samples[0].z.print();
// samples[1].z.print();

var t0 = _.now();
Optimize({
  model() { return model(images, batchSize); },
  optMethod: {adam: {stepSize}},
  estimator: {ELBO: {avgBaselines: false}},
  steps: numSteps,
  verbose: false,
  checkGradients: false,
  onStep(i, elbo) {
    display(i + ': ' + elbo / images.length);
  },
  onFinish(obj) {
    var history = obj.history;
    var elapsed = _.now() - t0;
    fs.writeFileSync('./benchmark/results/' + _.now() + '.json',
                     JSON.stringify({condition, elapsed, history}));
  }
});

//getParams();
'done';
